{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "HojYv92k8CxA",
   "metadata": {
    "id": "HojYv92k8CxA"
   },
   "source": [
    "# Semantic search over social media posts (tweets)\n",
    "\n",
    "This method performs a semantic search on a collection of social media posts, such as tweets, and identifies the posts most similar to a given search query. It leverages a pretrained language model (embeddings) to calculate semantic similarity using cosine similarity and provides ranked results based on relevance.\n",
    "\n",
    "The script is divided into these sections:\n",
    "\n",
    "__1. Environment Setup and Dependencies:__ This section imports all necessary utility functions and configurations.\n",
    "\n",
    "__2. Data Loading and Configuration:__  Here, the input query is set, and the dataset (social media posts) is loaded for the search process.\n",
    "\n",
    "__3. Getting the Document Embeddings:__\n",
    " This section processes the social media posts and transforms them into numerical embeddings.\n",
    "\n",
    "__4. Semantic Search through the Posts:__ Using cosine similarity, the script identifies posts most similar to the user-provided search query.\n",
    "\n",
    "__5. Output:__ The search results are displayed and saved in a JSON file for further analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30307edc-a521-4cd9-b094-846fe35c2693",
   "metadata": {},
   "source": [
    " ## 1. Environment Setup and Dependencies\n",
    "*Some utility functionalities regarding data loading, preprocessing, and tokenization are in the `utils.py` file.*\n",
    "\n",
    "We use Twitter samples downloaded from the NLTK library to demonstrate this method. You can download it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ab1e70-cc5e-497f-a72b-5787e32c426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd242ee-bbda-446e-8bbd-cd6df3cf0562",
   "metadata": {},
   "source": [
    "Now we import import internal (utils.py) and external resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f47f530-3d08-4548-aa46-163c67fc7553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/shyam/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    }
   ],
   "source": [
    "# import internal (utils.py) and external resources\n",
    "from utils import (load_data, clean_posts, tokenize_posts, \n",
    "cosine_similarity, read_configurations, write_output)\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4f083-2b1d-49fe-b4d8-089922fbebae",
   "metadata": {},
   "source": [
    "We predefine random seeds to ensure reproducibility of results across executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3775078-5731-4b47-91e1-a624ad3e990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predictable random numbers in reuse\n",
    "import random\n",
    "random.seed(13)\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac47f7-594e-43d1-a112-a508f5a0c4e4",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Configuration\n",
    "We now load the configurations from `/config.json`. This file defines the paths for the dataset and input data, the location for saving output results, and additional parameters to customize the method's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2eb4259-0c4e-4e8c-a6bd-4e4990e24b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configurations:  {'ifpreprocess': True, 'top-k': 5, 'input_query_filepath': '/data/input_queries.txt', 'output_filepath': '/data/output.json', 'posts_filepath': '/corpora/tweets.20150430-223406.json'}\n"
     ]
    }
   ],
   "source": [
    "configurations = read_configurations(\"/config.json\")\n",
    "print(\"configurations: \", configurations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0cf0a8-08ab-4c62-a148-d907a2b662db",
   "metadata": {},
   "source": [
    "Now, you can set your own input query directly. If you leave the `ls_input_queries` variable empty, the method will read the query from the input file defined in the configuration. The posts will be loaded from the file specified in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38699fac-a1c4-464f-8439-44e84e8e9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input queries\n",
    "ls_input_queries, ls_posts = load_data(configurations['input_query_filepath'], configurations['posts_filepath']) \n",
    "user_input = input(\"Provide your input query for search: \")\n",
    "\n",
    "if user_input != \"\":\n",
    "    ls_input_queries = []\n",
    "    ls_input_queries.append(user_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iqbAWxNiD4Jr",
   "metadata": {
    "id": "iqbAWxNiD4Jr"
   },
   "source": [
    "\n",
    "## 3. Getting the Document Embeddings\n",
    "\n",
    "###  The Word Embeddings Data for English Words\n",
    "\n",
    "The full dataset for English embeddings is about 3.64 gigabytes. To prevent the workspace from\n",
    "crashing, we've extracted a subset of the embeddings for the words that you'll\n",
    "use in this Tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83648f22-d092-42df-bef3-0c10508e5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "en_embeddings = pickle.load(open(\"./embeddings/en_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GQ8FFDW58CxI",
   "metadata": {
    "id": "GQ8FFDW58CxI"
   },
   "source": [
    "\n",
    "\n",
    "### Bag-of-words (BOW) Document Models\n",
    "Text documents are sequences of words.\n",
    "* The ordering of words makes a difference. For example, sentences \"Apple pie is\n",
    "better than pepperoni pizza.\" and \"Pepperoni pizza is better than apple pie\"\n",
    "have opposite meanings due to the word ordering.\n",
    "* However, for some applications, ignoring the order of words can allow\n",
    "us to train an efficient and still effective model. *In this method, we are averaging the word vectors in a post i.e., losing their position related information*\n",
    "* This approach is called Bag-of-words document model.\n",
    "\n",
    "### Document Embeddings\n",
    "* Document embedding is created by summing up the embeddings of all words\n",
    "in the document.\n",
    "* If we don't know the embedding of some word, we can ignore that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a17fb7d8-caca-4400-a802-d2255516ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess social media posts by removing urls, hashtags, stickers and other unwanted patterns.\n",
    "\n",
    "if configurations[\"ifpreprocess\"]:\n",
    "    posts = clean_posts(ls_posts)\n",
    "\n",
    "# Tokenize, stem and return clean tokens\n",
    "tokenized_posts = tokenize_posts(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geJAOUKx8CxI",
   "metadata": {
    "id": "geJAOUKx8CxI"
   },
   "source": [
    "<a name=\"1-1-4\"></a>\n",
    "\n",
    "### Function 'get_document_embedding'\n",
    "* The function `get_document_embedding()` encodes entire document as a \"document\" embedding.\n",
    "* It takes in a document (as a string) and a dictionary, `en_embeddings`\n",
    "* It processes the document, and looks up the corresponding embedding of each word.\n",
    "* It then sums them up and returns the sum of all word vectors of that processed tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9hUI39rP8CxJ",
   "metadata": {
    "id": "9hUI39rP8CxJ"
   },
   "outputs": [],
   "source": [
    "#Compute doc embedding vector i.e., average of all its word embeddings\n",
    "def compute_doc_embedding(post):\n",
    "    doc_embedding = np.zeros(300)\n",
    "    for token in post:\n",
    "        # add the word embedding to the running total for the document embedding\n",
    "        doc_embedding += en_embeddings.get(token, np.zeros(300))\n",
    "        doc_embedding = np.divide(doc_embedding, len(post))\n",
    "    return doc_embedding\n",
    "\n",
    "# This method reads a social media posts anc returns its embedded vector i.e., the average of embedded vectors of all its words\n",
    "def vectorize_posts(posts, en_embeddings):\n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - en_embeddings: a dictionary of word embeddings\n",
    "    Output:\n",
    "        - doc_embedding: sum of all word embeddings in the tweet\n",
    "    '''\n",
    "\n",
    "    # the dictionary's key is an index (integer) that identifies a specific tweet\n",
    "    # the value is the document embedding for that document\n",
    "    ind2Doc_dict = {}\n",
    "\n",
    "    # this is list that will store the document vectors\n",
    "    document_vec_l = []\n",
    "    \n",
    "    posts_embeddings = []\n",
    "    i = 0\n",
    "    for post in posts:\n",
    "        doc_embedding = compute_doc_embedding(post)\n",
    "\n",
    "        # save the document embedding into the ind2Tweet dictionary at index i\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "        i += 1\n",
    "        # append the document embedding to the list of document vectors\n",
    "        document_vec_l.append(doc_embedding)\n",
    "\n",
    "        # convert the list of document vectors into a 2D array (each row is a document vector)\n",
    "        document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EKzzc5-y8CxL",
   "metadata": {
    "id": "EKzzc5-y8CxL"
   },
   "source": [
    "<a name=\"1-1-5\"></a>\n",
    "### Function `'Vectorize_posts'`\n",
    "Now, let's store all the posts embeddings into a dictionary.\n",
    "Implement `vectorize_posts()`\n",
    "\n",
    "The following cell computes embedding (Posts x vector) matirx having each post represented with the standard 300 size vector. It may take *5-10mins* for *20,000 posts* on regular PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60e3ee01-00ff-46d0-a0f3-6e474b3cbc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized documents into their 300vector embeddings\n",
    "# The word vectors are averaged for each document\n",
    "\n",
    "posts_vec_matrix, ind2Doc_dict = vectorize_posts(tokenized_posts,en_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b971ce9e-a591-406d-af54-53e7b3fa092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 500\n",
      "shape of document_vecs (500, 300)\n"
     ]
    }
   ],
   "source": [
    "# ind2Doc dictionary and matrix of posts vectors generated\n",
    "\n",
    "print(f\"length of dictionary {len(ind2Doc_dict)}\")\n",
    "print(f\"shape of document_vecs {posts_vec_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8G8lDqU8CxM",
   "metadata": {
    "id": "c8G8lDqU8CxM"
   },
   "source": [
    "\n",
    "## 4 - Semantic Search through the Posts\n",
    "\n",
    "Now you have a vector of dimension (m,d) where `m` is the number of posts\n",
    "(20,000) and `d` is the dimension of the embeddings (300).  \n",
    "\n",
    "Now we will calculate post similarities for the query inputs using cosine similarity over the entire posts vector matix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70228993-5690-4d14-a5fb-1cafe28ca992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess queries.\n",
    "\n",
    "if False: #configurations[\"ifpreprocess\"]:\n",
    "    ls_input_queries = clean_posts(ls_input_queries)\n",
    "\n",
    "# Tokenize, stem and return clean tokens\n",
    "if False:\n",
    "    tokenized_queries = tokenize_posts(ls_input_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f8166e3-5ac6-4784-b793-eeab074a7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get top K similar posts for all query items\n",
    "\n",
    "query_posts_similarities = {}\n",
    "for query in ls_input_queries:\n",
    "    query = query.lower()\n",
    "    query = query.split(' ')\n",
    "    query_embedding = compute_doc_embedding(query)\n",
    "    cosine_score = cosine_similarity(posts_vec_matrix, query_embedding)\n",
    "    top_indices = np.argsort(cosine_score)[-configurations[\"top-k\"]:][::-1]\n",
    "    query_str = ' '.join(query)\n",
    "    query_posts_similarities[query_str] = []\n",
    "    top_posts = []\n",
    "    for idx in top_indices:\n",
    "        top_posts.append({'post ID': str(idx),\n",
    "                          'post text': ls_posts[idx], \n",
    "                          \"sim score\": str(cosine_score[idx])})\n",
    "    query_posts_similarities[query_str] = top_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2e536f1-dcbd-4df1-b00c-13278c9f870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json.dumps(query_posts_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47948a-b872-44dd-b16c-a5b7371578de",
   "metadata": {},
   "source": [
    "## 5. Output\n",
    "Now we save output in json format and you can read the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3ca3d7d-998f-4603-ada7-87d13a149d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_output(configurations['output_filepath'], json.dumps(query_posts_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a7266f7-df59-496c-9e43-af251abbcc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'social norms': [{'post ID': '471',\n",
       "   'post text': 'Valentine et al (2009) found relationships between homo/biphobic comments &amp; certain disciplines- incl. European languages, lit, education :(',\n",
       "   'sim score': '0.36462688735808163'},\n",
       "  {'post ID': '340',\n",
       "   'post text': \"Valentine et al found r'ships btwn homo/biphobic comments &amp; certain disciplines - incl. European langs, lit, education :( #fresherstofinals\",\n",
       "   'sim score': '0.3646158363561469'},\n",
       "  {'post ID': '436',\n",
       "   'post text': 'Last time I was here, was a funeral and a again funeral. Modimo ho tseba wena fela. :( — feeling emotional at... http://t.co/mQYsswdot7',\n",
       "   'sim score': '0.33736974710126716'},\n",
       "  {'post ID': '361',\n",
       "   'post text': 'facebook, y u no work ? y u do this facebook ? :(',\n",
       "   'sim score': '0.30875597566025537'},\n",
       "  {'post ID': '219',\n",
       "   'post text': 'why they cut the encore i wanna see snsd infinite interaction :(',\n",
       "   'sim score': '0.28145890015142466'}],\n",
       " 'community interaction': [{'post ID': '219',\n",
       "   'post text': 'why they cut the encore i wanna see snsd infinite interaction :(',\n",
       "   'sim score': '0.9334704047035272'},\n",
       "  {'post ID': '211',\n",
       "   'post text': 'their reactions :(((((',\n",
       "   'sim score': '0.3674853007944256'},\n",
       "  {'post ID': '29',\n",
       "   'post text': 'and my friends :(',\n",
       "   'sim score': '0.33448479578154344'},\n",
       "  {'post ID': '375',\n",
       "   'post text': 'Baggage claim. The final goodbye to all your new plane friends :(',\n",
       "   'sim score': '0.3336294119318698'},\n",
       "  {'post ID': '334',\n",
       "   'post text': \"@katjturgoose @artsjobs Link doesn't work! :(\",\n",
       "   'sim score': '0.2870397183785985'}],\n",
       " 'cultural identity': [{'post ID': '87',\n",
       "   'post text': 'I wish I had my own Baymax :(',\n",
       "   'sim score': '0.26428523895624095'},\n",
       "  {'post ID': '413',\n",
       "   'post text': \"Sometimes it be's like that, yo. Follow someone and then a few days later realise they're problematic as fuck. Life :(\",\n",
       "   'sim score': '0.2536635782181117'},\n",
       "  {'post ID': '156',\n",
       "   'post text': '@megsyfoxie WTF LIKE YOUR LIPS STILL A VIRGIN OMG MOM GET A LIFE YOURE 45 \\n\\nHahah : (',\n",
       "   'sim score': '0.2521975012292207'},\n",
       "  {'post ID': '331',\n",
       "   'post text': \"I don't want to be in a world where Hulk Hogan has been scrubbed from WWE history :-(\",\n",
       "   'sim score': '0.24855694223372138'},\n",
       "  {'post ID': '295',\n",
       "   'post text': \"My mom's a linguist. My dad's a computer scientist. And I am the dumbest one in the family :-(\",\n",
       "   'sim score': '0.24849331110916906'}],\n",
       " '': [{'post ID': '499',\n",
       "   'post text': '@Wapaseeto yes, all wasp stings have been averted. Those bugs are mean  :(',\n",
       "   'sim score': '0.0'},\n",
       "  {'post ID': '170',\n",
       "   'post text': '@selenagomez i miss your tweet spree oh :(',\n",
       "   'sim score': '0.0'},\n",
       "  {'post ID': '157',\n",
       "   'post text': '@pastelwolfxx @CHA_NNNNN niNASTY tsktsk oppa wont like u call his dick kawaii he is manly &gt;:( http://t.co/83cUgt7qQ3',\n",
       "   'sim score': '0.0'},\n",
       "  {'post ID': '158',\n",
       "   'post text': 'So Lonely :( http://t.co/VM4N0n8Bkw',\n",
       "   'sim score': '0.0'},\n",
       "  {'post ID': '159',\n",
       "   'post text': \"can't go back to sleep :((\",\n",
       "   'sim score': '0.0'}]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_posts_similarities"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
